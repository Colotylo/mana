CONTENTS:
 * General notes for building, testing and running on Cori supercomputer
 * Debugging internals of MANA
 * Building and testing on ordinary CentOS/Ubuntu (not the Cori supercomputer)

================================================================================
General notes for building, testing and running on Cori supercomputer:

The main pointers for background information on MANA are:

MANA at github:
  https://github.com/mpickpt/mana/tree/refactoring
  (This is the refactoring branch of https://github.com/mpickpt/mana;
   soon, this should be merged into the 'master' branch.)
and MANA documentation:
  https://docs.google.com/document/d/1pT25gvMNeT1Vz4SU6Gp4Hx9MGLfkK8ZK-sSOwtu5R50/edit

Eventually, we'll have scripts for this:
  mpi_mana_launch, mpi_mana_restart, mpi_mana_status.
But for now, we're still modifying MANA.

At this time, the steps to build MANA (on Cori) are roughly as follows.

  module unload craype-hugepages2M
  # Modify the salloc command according to your needs:
  salloc -N 1 -C haswell -q interactive -t 03:00:00
  git clone https://github.com/mpickpt/mana
  git checkout refactoring  # current development branch; later to be 'master'
  cd mana
  # Development branch.  (Eventually, the 'master' branch will be preferred.)
  git checkout refactoring
  ./configure
  make -j  # Build in source directory: create bin, lib directories.
  pushd contrib/mpi-proxy-split  # Go to plugin directory.
  (cd test && make -j)
  make install
  pushd  # Go back to the MANA root directory
  make -j  # Make it again. The plugin 'make' added an extra file to core DMTCP.
  pushd  # Go back to plugin directory: mpi-proxy-split
  cd test # We're now in:  contrib/mpi-proxy-split/test

Roughly, to test MANA, do:
  export MANA_ROOT=$HOME/mana  # Adjust for your directory.
  cd $MANA_ROOT/contrib/mpi-proxy-split/test
  make tidy # Remove old ckpt files from a previous session.
  $MANA_ROOT/bin/dmtcp_coordinator --mpi -q -q --exit-on-last --daemon
  # This does interval checkpointing every 5 seconds.
  srun -n 1 $MANA_ROOT/bin/dmtcp_launch -i5 --host `hostname` --no-gzip --join --disable-dl-plugin --with-plugin $PWD/../libmana.so ./mpi_hello_world.mana.exe
  # [ creates ckpt:   ls ckpt_rank_*/ckpt_*.dmtcp ]
  $MANA_ROOT/bin/dmtcp_coordinator --mpi -q -q --exit-on-last --daemon
  srun -n 1 $MANA_ROOT/bin/dmtcp_restart --host `hostname` --join --mpi ckpt_rank_*/ckpt_*.dmtcp

To see if a coordinator (or other process) is already running do:
  ps uxw

We intend to later simplify all of this with scripts for
  mpi_mana_launch, mpi_mana_restart, and mpi_mana_status.
But right now, MANA is still under development.

================================================================================
Debugging internals of MANA:

For either command line with srun (or mpirun), prefix the command line
with 'DMTCP_MANA_PAUSE=1' and run the command in the background:
  DMTCP_MANA_PAUSE=1 srun/mpirun ... &
It will stop with a printout about how to attach in GDB.  Execute that command.
Then:  (gdb) p dummy=0

If you're debugging dmtcp_launch:
  DMTCP_MANA_PAUSE=1 srun -n 1 $MANA_ROOT/bin/dmtcp_launch ...
then it will drop you inside dmtcp_launch.  To reach, for example,
mpi-proxy-split/split-process.c:splitProcess(), then do:
  (gdb) b execvp
  (gdb) continue
  (gdb) b splitProcess
  (gdb) continue
This is in a global constructor that executes before main.  To reach 'main'
of the target executable, do 'b main' instead.

Similarly for dmtcp_restart, MANA_RESTART_PAUSE=1 drops you in
  src/mtcp/mtcp_restart.c shortly before mtcp_restart.c:splitProcess().
If you want to reach the target executable, use DMTCP_RESTART_PAUSE=1 instead.

When debugging in GDB, you can switch back and forth between upper half and
lower half programs as follows (assuming you're in .../mpi-proxy-split/test).
This is useful for examining both the upper and lower half stacks.
  (gdb) file ../../../lh_proxy
  (gdb) file ../mpi_hello_world.mana.exe
If you had previously set a breakpoint before switching, then use:
  (gdb) set breakpoint always-inserted on
before switching among lower and upper halves.

================================================================================
Building and testing on ordinary CentOS/Ubuntu (not the Cori supercomputer):

Before going into details, there are a few prerequisites:
A.  We have only tested with MPICH, not Open MPI.
B.  You will need to build your own MPICH package.  The MPICH package
    supplied with Ubuntu (or CentOS??) is unlikely to work.
    That package does not seem to work well with the '-static' flag
    required for MANA.  In addition, that package seems to have been
    configured to require '-lcr' (BLCR).  BLCR is not actively supported,
    and 'mpicc -static' with the MANA options is likely to fail due to
    interactions with '-lcr'.
C.  The mpicc command of MPICH tests if libxml2.so is present, and if so,
    it appends a flag '-lxml2'.  As is clear, this flag fails during
    MANA's use of 'mpicc -static' if libxml2.a has not been installed.
    If you have installed the package 'libxml2', then you _must_ also
    install 'libxml2-dev' (libxml2-devel in CentOS).
    (If you are missing libxml2.a and lack root privilege, try copying libxml2.a
     from some distro package, and add -LPATH to mpicc command in ${PROXY_BIN}
     target of lower-half/Makefile, where PATH is a path to your libxml2.a .)
D.  If you try MANA with Open MPI, please give us feedback on your results.

Building outside of Cori (details).
1.  git checkout test-dekaksi    # This will be merged into refactoring later.
2.  Instead of './configure', do 'sh configure-mana'.  You will probably
    have to modify the 'configure-mana' script to point to your MPICH build.
3.  NOTE:  Using 'mpicc -static' seems to be problematic for MPICH, as
            distributed with Ubuntu and CentOS.
            In mpi-proxy-split/lower-half/Makefile, it tries to fix that.  If
            this fails for you, see comments for PROXY_BIN target of Makefile.

===
Testing and running is mostly the same as described at the top.
When testing at your site, you may need to use 'mpirun' instead of 'srun'
in the two commands for dmtcp_launch and dmtcp_restart..

A further complication tends to occur when using 'mpirun' on MPICH-derived
MPI implementations.  The lower half, lh_proxy, was compiled statically
with libc.a.  However, libc.a does not directly support gethostbyname
and getaddrinfo.  Instead, it tries to load libnss_*.so.  But we wanted
lh_proxy to be statically linked so that it would consist only of
text/data/stack, and later calls to mmap.

(This tends not to occur on HPC clusters, where we are more likely to
use 'srun', and anyway, where the network interface will be a custom
one (InfiniBand, Cray GNI, etc.).  So, it is less likely that libc.a
calls libnss_*.so.)

The workaround for 'mpirun' (at least for the MPICH-based family
of implementations) is to use:
  ip addr | grep -B1 link/ether  # and choose one of thsoe interfaces
OR ELSE:
  ifconfig -a (deprecated)
to discover what network interfaces are available on the HPC cluster.
Then call mpirun during launch and restart with an additional flag:
  mpirun -iface eth0 ...
(or with your network interface, if 'eth0' is not listed)
NOTE: Open MPI seems to auto-detect an appropriate network interface,
      but we have not tested Open MPI to confirm this so far.
